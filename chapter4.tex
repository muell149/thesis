%
% Chapter 4
%

\chapter{PHYSICS OBJECTS}
\label{chap:physics_objects}
Each of the CMS subdetectors (neglecting the trigger system) technically only record and detect hits and energy deposits. While these hits and energy deposits are almost
always due to passing particles, the detectors and readouts themselves only produce information about the position, multiplicity, and value of
these hits and energy deposits respectively. Sophisticated algorithms are used to reconstruct these hits and deposits into particles. These reconstructed particles are
referred to as physics \emph{objects}. The reconstruction techniques vary greatly with different objects and suit the subdetectors used to detect and record their
hits and energy deposits. 

\section{Object Reconstruction and Particle Flow}
The particle flow algorithm is used by CMS to reconstruct physics objects from hits and energy deposits. Particle flow and CMS are unique in the sense
that nearly all physics analyses performed on data collected by CMS use objects reconstructed with this approach.
Particle flow coordinates the many reconstruction algorithms across all CMS subdetectors into a single global picture of each event. The primary advantage of this strategy is
uniform and consistent object definitions across nearly all papers published on behalf of CMS. The purpose of particle flow is to
identify all final-state stable particles in an event recorded by CMS, specifically electrons,
muons, taus, jets, and photons. Particle flow optimally combines building-block information (hits and energy clusters) from all subdetectors to reconstruct objects and
determine particle type, position, and momentum. To accomplish this, two different primary reconstruction techniques are used. One for reconstructing tracks from tracker hits,
and one for clustering energy deposits from individual calorimeter cells. 

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.8\textwidth]{ch4_figs/cms_particleflow.pdf}
   \caption[CMS slice in the x-y plane]{An overview of how CMS detects different types of particles. The slice of CMS in in the x-y plane.~\cite{cms_pflow_img}.}
   \label{fig:cms_pflow}
 \end{center}
\end{figure}

Object reconstruction begins with grouping collections of hits into tracks in an iterative process~\cite{CMS-TRK-09-001}. In the first iteration, tracks are
seeded with initial hits and subject to very tight criteria, sacrificing efficiency for a low fake rate. In the following iterations, hits assigned to
tracks in the previous iteration are removed from further consideration, and the criteria for candidate tracks is gradually relaxed with each iteration. In the final iterations,
the constraints on the track seed are relaxed to account for secondary decays from photon conversions and nuclear interactions with the silicon tracker material. This technique
reconstructs tracks with as few as three hits and \pt as small as 150 MeV with a fake rate in the single digits~\cite{CMS-PFT-09-001}. A similar but separate track
reconstruction is performed in the muon chambers, however the minimum muon \pt is around 4 GeV. 

Object reconstruction continues in the calorimeters, where it relies on a clustering algorithm to identify individual energy deposits and associate them to an object. This
algorithm is designed to yield a high efficiency even for low energy, and nearby objects. The clustering process is performed separately in the ECAL and HCAL, and furthermore in
the barrel, and endcaps. The ECAL preshower also uses separate clusterings in each of its two layers. No clustering is performed in the HF, where each module is an
independent cluster. The clustering algorithm begins by identifying cells (cluster seeds) with an energy above a ``seed'' energy threshold. Clusters are then increased by
adding adjacent\footnote{sharing at least one edge with the seed} cells that have an energy above a given threshold. The calorimeter granularity is used to optimize the
determination of cluster energies and positions~\cite{CMS-PFT-09-001}.  

After the tracking and clustering is complete, Particle Flow then matches tracks in the tracker to energy clusters in the calorimeters, and to tracks in the muon chambers.
In a given event, there can be many different objects and Particle Flow utilizes
a process-of-elimination strategy. Because they are the easiest to identify unambiguously, muons are identified first by matching tracks in the inner tracker to the tracks in
the muon chambers. The matching criteria is based on a $\chi^{2}$ fit threshold. When multiple sets of tracks are matched, the set with the lowest $\chi^{2}$ is selected as the
muon object. Muons are first reconstructed this way and called global muons~\cite{globalmuon}. Particle flow muons are identifed from global muons when the \pt measurements
in the tracker and the muon chambers agree to within three standard deviations. The tracks corresponding to the muon object are then removed from consideration for the
remaining object reconstruction. 

Electrons are reconstructed next. Electrons emit characteristic Bremsstrahlung radiation due to their small mass as they are deflected in the magnetic field.
This Bremsstrahlung radiation is emitted tangentially to the deflected electron's track, and is subsequenctly detected in the ECAL. 
Tracks are flagged based on these characteristics as potential electron candidates and refitted with a Gaussian-Sum Filter (GSF)~\cite{gsf} and the
resulting tracks are then matched to ECAL energy clusters. The ECAL clustering and track matching accounts for the Bremsstrahlung radiation in electron object reconstruction.
These matches are then subject to additional quality criteria before they are considered particle flow electrons, and their
corresponding tracks and energy deposits removed from further consideration for the remaining object reconstruction. 

At this point in the event/object reconstruction, the so-called ``low hanging fruit'' has been picked, and the more difficult objects are all that remain. These objects are charged
and neutral hadrons (jets), and photons. The neutral particles are difficult to reconstruct because they don't leave hits in the tracker, so only calorimeter information is
available. Photons interact electromagnetically and are stopped by the ECAL, while neutral hadrons are stopped and deposit their energy in the HCAL.
The remaining tracks are subject to
quality cuts aimed at reducing the fake-rate. The high-quality tracks passing these thresholds are matched to ECAL and HCAL deposits, and give rise to particle flow 
charge hadrons. The momentum of these objects is measured from the track radius and compared to the corresponding energy deposit in the calorimeters assuming the object is
a charged pion. If the two measurements are compatible, the momenta is refined with additional fits to the tracks and energy deposit(s)~\cite{CMS-PFT-09-001}.

The charged and neutral hadron objects with tracks and matched energy deposits are only considered particle flow candidates at this stage.
An additional clustering step is necessary to reconstruct jet objects. As mentioned previously, when free quarks produced in the pp collision decay in the
fragmentation/hadronization process, they produce energetic and collimated sprays of charged
and neutral hadrons, called jets. To accurately determine the direction and momentum of the initial quark, as many of the corresponding charged and neutral hadrons as possible must not only be
reconstructed correctly and identified, but also clustered together (matched) correctly.
These jets are then interpreted experimentally to be the free quark. A depiction of the hadronization and jet detection process is below in Figure~\ref{fig:frag}.

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.8\textwidth]{ch4_figs/jet_frag.pdf}
   \caption[Jet hadronization example]{An example of quark hadronization and the resulting jet.~\cite{frag}.}
   \label{fig:frag}
 \end{center}
\end{figure}
 
Because the hadronization process is challenging to reconstruct, jet clustering is accomplished with specialized algorithms. 
These algorithms exploit information from the detector with theoretical knowledge of the
hadronization process to cluster the jets in a sensible and reproducible manner.
While many clustering algorithms exist, CMS uses the anti-$k_{T}$ algorithm~\cite{antikt} for jet clustering.
Like other sequential clustering algorithms, anti-$k_{T}$ begins by finding the highest \pt candidate or seed, and
calculating the distance measures in equations~\ref{eqn:antikt1}. 

\begin{equation}
\begin{aligned}
\label{eqn:antikt1}
%%\begin{split}
d_{ij} &= min(k^{2p}_{Ti},k^{2p}_{Tj})\frac{\Delta_{ij}^{2}}{R^{2}} \\ d_{iB} &= k^{2p}_{Ti}
%%\end{split}  
\end{aligned} 
\end{equation}

\noindent where $\Delta_{ij}^{2} = (y_{i}-y_{j})^{2} + (\phi_{i}-\phi_{j})^{2}$ and $y_{i}$, $k_{Ti}$ are the rapidity and transverse
 momenta of candidate i respectively. After the distance measures have been calculated for each candidate in the event,
the smallest $d_{ij}$ are merged into one object by summing the 4-momenta of particles i and j, the
distance measures are updated and the algorithm moves onto the next smallest $d_{ij}$. If a particle has the smallest $d_{iB}$, it is removed and called a jet. This
iterative process continues until all PF candidates are clustered into jets.

Conceptually, the exponents with $p$ dictate how transverse momenta varies with distance parameter. Negative values of $p$ merge higher transverse momenta, nearby 
candidates first. What differentiates anti-$k_{T}$ from Cambridge-Aachen and other similar algorithms is the choice
of $p$ = -1 in equation~\ref{eqn:antikt1}. This negative value explains the name of the algorithm, and the tendency for it to produce circular\footnote{circular in the
$\eta$-$\phi$ plane} jets, centered on the highest \pt~candidate of the jet. The effect of these parameter values with respect to other available algorithms is below in
Figure~\ref{fig:jet_cluster}.

\begin{figure}[htbp] 
  {\centering
    \subfigure[$k_{T}$]{\includegraphics[width=0.4\textwidth]{ch4_figs/kt.pdf}}
    \subfigure[Cambridge-Aachen]{\includegraphics[width=0.4\textwidth]{ch4_figs/ca.pdf}}
    \subfigure[SIScone]{\includegraphics[width=0.4\textwidth]{ch4_figs/sis.pdf}}
    \subfigure[anti-$k_{T}$]{\includegraphics[width=0.4\textwidth]{ch4_figs/akt.pdf}}
    \caption[A comparison of jet clustering algorithms]{Jets in a sample MC event clustered with various algorithms.
    \label{fig:jet_cluster}}
\end{figure}

The final parameter in equation~\ref{eqn:antikt1} is defined as $R = \sqrt{\Delta\eta^{2}+\Delta\phi^{2}}$ and is the conesize of the jet being clustered.
The cone size used for clustering in this analysis is R = 0.4. Versions of this analysis on 7 TeV, and 8 TeV datasets used a wider cone of 0.5. The move to smaller cone size
was motivated by the fact that jets tend to be more energetic and thus narrower and more collimated as the center-of-mass collision energy increases to 13 TeV. All techniques
mentioned above provide CMS analyzers the basic physics objects needed to perform analysis, and also standardizes the object reconstruction across the experiment. 

\section{Primary Vertex Identification and Pile-up}
Due to the way the LHC collides bunches, there are multiple pp collisions in each LHC bunch crossing at CMS. Unfortunately, many of these collisions produce
multiple objects that are reconstructed, but at most one of these collisions is a hard scatter head-on collision capable of producing a \tth event.
These additional collisions that don't include the process of interest, as well as their resulting reconstructed objects are referred to as pileup, because
they are said to \emph{pile up} on top of the objects from the collision of interest.
The location of the collision of interest is called the primary vertex.
In this analysis, the primary vertex is defined as the vertex with the highest \pt~sum of all constituent tracks.  
Pileup is problematic because it can make matching objects to collisions difficult. Fortunately,
CMS has an excellent tracker which makes the process of matching tracks to vertices (collisions) fairly straightforward. Aside from the tracking, pileup is also
problematic because the energy deposits in the calorimeters from the pilup objects can distort the energy measurements and clustering of objects originating from
the primary vertex. There are numerous techniques to account for these effects. The technique employed in this analysis is to calculate the energy
due to pileup in the detector, rho, and subtract it off from each reconstructed jet energy measurement. This is called the rho correction.  
An example of the reconstructed tracks and vertices in an
LHC bunch crossing is below in Figure~\ref{fig:pileup_vertices}.

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.8\textwidth]{ch4_figs/cms_pileup.pdf}
   \caption[Pileup vertices in the CMS tracker]{A side view of the CMS tracker's reconstructed vertices in a special high pileup LHC run. The pileup here is 78, meaning there are
     78 collisions in a single bunch crossing~\cite{pileup_image}.}
   \label{fig:pileup_vertices}
 \end{center}
\end{figure}

\noindent The peak pileup values in the data analyzed vary from 30 to 45, meaning there are, on average, between 30 and 45 collisions (including the primary vertex) in each
bunch crossing recorded by CMS\footnote{For a given bunch crossing, the actual number of pileup collisions varies according to a Poisson distribution, where the average is taken.}. 

\section{Object Selection}
After the basic object reconstruction is performed, each event is ready for the first steps of analysis. The analysis begins with object selection where the objects in each
event are subjected to tighter criteria to ensure quality objects in the analysis that are consistent with signal expectations and reject background. This selection is tailored
to the type of objects in the desired signal event. Because the \tth multilepton final state is so complicated, almost\footnote{all objects except the photon, which are
used in the \tth,$H\rightarrow\gamma\gamma$ search} all objects available from the particle flow
reconstruction are needed to identify events that are consistent with the signal, namely jets, missing energy, charged leptons, and hadronic taus.

\subsection{Jets}
Jets are clustered with anti-kt and reconstructed from PF candidates as described previously using the FASTJET algorithm~\cite{antikt}\cite{fastjet}.
Charged hadrons not originating from the primary vertex are subtracted from the jet clustering. Because the detector performance varies with time the measured jet energies
must be updated to correct for performance degradation in the calorimeters. The jets are corrected in bins of jet $E_{T}$ (transverse energy) and $\eta$~\cite{jec}.

The jet selection for this analysis requires jets to have a \pt $>$ 25 GeV and $|\eta|$ $<$ 2.4. Jets are removed when they overlap with a fakeable lepton (described later) within
a $\Delta$R cone of $<$ 0.4. Finally, the jets must pass the working point of an MVA used to discriminate against jets from pileup vertices. This discriminator uses inputs
characterizing the shape of the jet, the relative amounts of charged and neutral candidates within the jet, and the \pt ratio between the candidates. 

\subsection{b-jet Identification}

Identifying jets from b-quark hadronization (b-jets) is crucial to selecting \tth events since the tops almost always produce b-quarks.
The properties of heavy flavor quarks, specifically the bottom quark, have unique characteristics which make it possible to tag and identify their
hadronizations in the detector. This identification is called
b-tagging and is used throughout this and many other analyses. These heavy flavor quarks are characterized and distinguished from light flavor
 by large\footnote{large relative to light quarks and gluons} masses,
long lifetimes, high \pt decay products, among which there is an occasional charged lepton.
The long lifetime provides a key handle for identifying b-decays, because the b quark is further displaced from the primary vertex before hadronizing,
resulting in a secondary vertex. Identifying this secondary vertex plays a central role in tagging jets from b-decays.

The algorithm used to identify b-jets in this analysis is the Combined Secondary Vertex (CSVv2) tagger~\cite{csvv2}. The CSVv2 tagger is an MVA discriminator that
combines secondary vertex information with impact parameter variables that discriminate heavy flavor quarks from light flavor quarks and gluons. The tagging variable is assigned
for each jet, and can range from -1 to +1, where the higher values correspond to higher likelihood that the jet originated from a b-quark. The jets described in the previous
subsection are \emph{tagged} when they pass specific working points of the tagger. The specific working point is choosen to balance keeping the fake rate low, and the efficiency
high. The working points used in this analysis were selected after studying the tagger performance, as a function of jet \pt and $\eta$ in data samples with b-jets. Two working points
are used for b-tagging in this analysis. The medium working point (CSVv2 $>$ 0.8484) corresponds to 70\% efficiency for tagging
b-jets with a 1.5\% mistag rate, while the loose working point (CSVv2 $>$ 0.5426) corresponds to an 85\% efficiency and a 10\% mistag rate. The mistag rate is the probability
to tag jets originating from light flavor quarks or gluons. The motivation for two working points is explained in Chapter~\ref{chap:event_selection}. 

\subsubsection{b-jet Scale Factors}
The shape of the CSVv2 distribution of b-jets in data differs somewhat between that of MC. Because MC is being used to predict both the signal and many of the backgrounds, we apply scale factors (SFs)
to correct the shape in the MC to match that of data. These scale factors are derived separately for heavy (b,c quark) flavor and light flavor as a function of the jet CSVv2, \pt, $|\eta|$. The SF is 
calculated according to equation~\ref{eqn:csvsf} below. 

\begin{equation}
\label{eqn:csvsf}
  SF(CSVv2,p_{T},\eta) = \frac{DATA-MC_{A}}{MC_{B}}
\end{equation}

\noindent The scale factor for heavy flavor has $MC_{A}$, $MC_{B}$ = light, heavy flavor MC respectively,
while the scale factor applied to light flavor has $MC_{A}$, $MC_{B}$ = heavy, light flavor MC respectively.
The scale factors are derived from a tag-and-probe technique. First,
a control region is defined by selecting opposite sign $e-\mu$ events with exactly two jets. When this selection is applied to data, it yields events enriched with di-leptonic \ttbar decays where the
two jets are most likely b-jets. Then one of the jets (the \emph{tag}) is required to pass the medium CSVv2 working point (CSVv2 $>$ 0.8484) to increase the \ttbar purity. In real \ttbar events, the
remaining jet (the \emph{probe}) is very likely a b-jet. Because control region applied to data is only \emph{enriched} in \ttbar events and not pure \ttbar events, we subtract off the contribution of
light flavor jets as estimated in MC, which explains the second term in the numerator of equation~\ref{eqn:csvsf}. Now we can directly apply this same control region to pure \ttbar MC, and the ratio
of the corrected yields in data to the pure \ttbar MC gives the scale factor described in equation~\ref{eqn:csvsf}. The same process is used for the light flavor correction, but the control region
definition is modified to instead select dileptonic events with two leptons that have the same flavor and opposite sign, with exactly two jets where one of the jets \emph{fails} the medium CSVv2 WP.
This control region applied on data yields events enriched in Z+jets and the tag-and-probe process is repeated. The full details of this technique are described in~\cite{CMS-AN-2013-130}~and~\cite{csvsf_twiki}.

\subsection{Missing Energy}
Missing energy, or more accurately missing \emph{transverse} energy is calculated as the negative vector sum of \pt of all PF candidates in the event\footnote{The word ``Energy'' is used here in place of momentum
for historical reasons. This quantity used to be calculated from the energy measured in the calorimeters of older experiments, and still is for neutral objects where momentum information is unavailable due to the
lack of a curved track.}, denoted as \met. It is called missing because
the gluons (or quarks) colliding in the hard scatter that produce events such as \tth have no initial transverse momentum. By momentum conservation, the vector sum of all particles
produced in the collision should match the sum before the collision. Any time the \met is non-zero, it is assumed to be either from mismeasurements, or carried off by undetected ``missing'' particles.
In the context of this and many other analyses, the \met is interpreted as the presence of neutrinos. The two same-sign lepton requirement on the signal ensures that there will always be two neutrinos in the event, making it impossible to fully
reconstruct the individual neutrino momenta. The presence of \met can also occur due to mismeasurements/incorrect object reconstructions. To discriminate
between the different sources of \met we use another variable called \HT, which is the negative vector sum of all selected jets and leptons in the event. While \HT has lower resolution than the \met,
it relies only on the higher \pt selected objects and not the softer objects failing the selection. To exploit the fact that \HT and \met are less correlated in events with incidental missing energy,
and more correlated in events with real missing energy, a linear discriminant of both varibles is defined in equation~\ref{eqn:metld} below.

\begin{equation}
\label{eqn:metld}
 \met LD = \met\times0.00397 + \HT\times0.00265.
\end{equation}

\noindent The coefficients are tuned to scale the \met term by 60\% and the \HT term by 40\%, which were empirically found to deliver the best signal efficiency and background rejection.
The \met LD threshold is described in Chapter~\ref{chap:event_selection}.

\subsection{Leptons}
The flagship objects that characterize this analysis are leptons. The lepton selection is the foundation of this \tth search and drives an important component of the background estimation.
The ultimate goal of the lepton selection is to identify and select prompt leptons and reject non-prompt leptons, also known as fake leptons. In this analysis, leptons are defined as being electrons or muons.
Taus are excluded from this defnition because they are unstable and decay in the detector. In this context, a prompt lepton
is a lepton that originates directly (promptly) from a W, Z or $\tau$ decay, while non-prompt leptons
predominantly originate from b-hadron decays, but also in-flight decays of pions, and photon conversions\footnote{Prompt leptons produced in $\tau$ decays are actually coming from an offshell W produced by the decaying $\tau$} .
The term \emph{fake} is used to describe non-prompt leptons because non-prompt leptons
that pass the criteria designed to select prompt leptons are faking prompt leptons. These fakes are one of the largest backgrounds in this analysis and the lepton selection is designed to reduce both
the quantity of fakes entering the signal regions, and the systematic uncertainties on that quantity. 

\subsubsection{Electron Identification}
Electrons are reconstructed from tracker hits (GSF tracks) and ECAL clusters via particle flow as described previously. While many other requirements are placed on electrons later, they first must
have $|\eta|<$2.5 to guarantee they passed through the tracker, and minimum \pt $>$ 7 GeV. Electrons must pass the working point of an MVA designed to identify electrons using shower-shape variables,
track-cluster consistency variables, and track quality variables~\cite{elemvaid}. We apply loose cuts as a function of $|\eta|$ on this MVA value. The value of this MVA discriminator is used as an input to
the Lepton MVA, which will be described in the following sections. For the tightest selections, we also require the charge measurements in the pixels and strips to agree and be well-matched to ECAL clusters.
Additionally, electrons must not have missing hits in the inner tracker, and pass additional veto which suppresses contributions from photon conversions. 


\subsubsection{Muon Identification}
Muons are reconstructed by combining tracks from the silicon tracker with tracks in the muon chambers and are required to be global muons. Muons are first required to have \pt $>$ 5 GeV and
$|\eta|<$2.4. We use the selectrion criteria developed by the CMS
Muon Physics Object Group (POG) and require all muons to pass the Loose ID~\cite{loosemuon} and in some cases the Medium ID~\cite{mediummuon} depending on the selection. Additionally, we require
the sign of the muon electric charge to be well measured by appling a cut related to the significance of the muon \pt measurement called \emph{tight charge} defined as: $\Delta~p_{T}/p_{T} < 0.2$,
where $\Delta~p_{T}$ is the uncertainty on the muon \pt. This cut ensures a high confidence on the charge sign of the muons. This cut is only required on the tightest selection of muons.  

\subsubsection{Isolation}
Lepton isolation is a measure of how spatially separated the lepton is from other physics objects in an event. Prompt leptons are typically isolated from hadronic activity
(jets) in the event, while non-prompt leptons, which are often produced in hadronic decays, are significantly less isolated and close to or overlapping with jets. The standard isolation considers
all charged and neutral hadrons as well as photons within a fixed cone size in R around the lepton. Then if there is too much energy relative to the lepton energy from the
other objects inside the fixed cone radius, the lepton fails the isolation criteria. This analysis uses a variation of this technique that varies the cone size with the \pt of the lepton, since
boosted (very high momentum) objects tend to be more collimated and thus should have a more collimated cone definition. This isolation technique is called mini isolation (\miniIso) and the cone
size is varied with lepton \pt according to figure~\ref{fig:miniIsoConeSize} below. 

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.8\textwidth]{ch4_figs/miniIso.pdf}
   \caption[Mini isolation cone size vs \pt]{Mini isolation cone size vs lepton \pt. For \pt $<$ 20 GeV, R = 0.2, for \pt $>$ 200 GeV, R = 0.05.~\cite{miniIso}.}
   \label{fig:miniIsoConeSize}
 \end{center}
\end{figure}

%\begin{equation}
%\label{eqn:miniIsoConeSize}
% R = \frac{10}{min(max(p_{T}(l),50),200)}
%\end{equation}

\noindent The amount of activity inside this cone is adjusted to correct for pileup using the $\rho$-correction, which uses a specific pileup-density event-by-event in different $|\eta|$ ranges in the detector. The
pileup density $\rho$, is multiplied by effective areas as a function of $|\eta|$. The \miniIso quantity is defined in equation~\ref{eqn:miniIso} below

\begin{equation}
\label{eqn:miniIso}
 I_{mini} = \frac{\sum_{R}p_{T}(h^{\pm}) - max(0,\sum_{R}p_{T}(h^{0}) + p_{T}(\gamma) - \rho A (\frac{R}{0.3})^{2}) }{p_{T}(l)}
\end{equation}

\noindent where $\sum_{R}p_{T}(h^{\pm})$ is the energy of the charged hadrons inside the cone of radius R, $\sum_{R}p_{T}(h^{0})$ is the energy of the neutral hadrons inside the cone, $p_{T}(\gamma)$ is
the energy from photons inside the cone, and $-\rho A (\frac{R}{0.3})^{2}$ is the correction for pileup. The effective areas, A, are defined separately for muons and electrons and are based on a fixed
cone size of R = 0.3 and listed in tables~\ref{tab:ele_effArea}\ref{tab:mu_effArea}, which explains the denominator in the pileup correction term. Both muons and electrons are subject to the same isolation criteria,
which requires \miniIso $<$ 0.4.

\begin{table}[hbtp]
\centering
\caption[Electron effective areas for the pileup correction.]{Electron effective areas for the pileup correction.}
\begin{tabular}{lcc}
\hline
$|\eta|$ range & effective area (A) \\
\hline
$|\eta| < 1.0$ & 0.1752 \\
$1.0 < |\eta| < 1.479$ & 0.1862 \\
$1.479 < |\eta| < 2.0$ & 0.1411 \\
$2.0 < |\eta| < 2.2$ & 0.1534 \\
$2.2 < |\eta| < 2.3$ & 0.1903 \\
$2.3 < |\eta| < 2.4$ & 0.2243 \\
$2.4 < |\eta|$ & 0.2687 \\
\hline
\end{tabular}
\label{tab:ele_effArea}
\end{table}

\begin{table}[hbtp]
\centering
\caption[Muon effective areas for the pileup correction.]{Muon effective areas for the pileup correction.}
\begin{tabular}{lcc}
\hline
$|\eta|$ range & effective area (A) \\
\hline
$|\eta| < 0.8$ & 0.0735 \\
$0.8 < |\eta| < 1.3$ & 0.0619 \\
$1.3 < |\eta| < 2.0$ & 0.0465 \\
$2.0 < |\eta| < 2.2$ & 0.0433 \\
$2.2 < |\eta|$ & 0.0577 \\
\hline
\end{tabular}
\label{tab:mu_effArea}
\end{table}

\subsubsection{Vertexing}
Vertexing requirements are placed on the leptons to help to ensure they are coming from (associated with) the primary vertex and to
remove leptons from misreconstructed tracks and hadron decays from further consideration. The requirements
placed on the leptons include the impact parameter in both the transverse plane ($d_{xy}$), the z-direction ($d_{z}$) and the significance
on the three-dimensional impact parameter (SIP$_{3D}$). The impact parameter is the distance of closest approach of the lepton track to the primary vertex.
The values of thes cuts are detailed in tables~\ref{tab:muonIDs}~and~\ref{tab:eleIDs}.
These cuts are designed to reduce the contribution of pileup and misreconstructed tracks. All of these variables are useful in distinguishing prompt from non-prompt leptons
and are inputs to the lepton MVA. 


\subsubsection{Jet-related Variables}
In order to reduce the contribution of fake leptons from hadron decays, specifically b-decays, we construct several variables that incorporate
the characteristics of the nearest jet to the selected lepton. Using the jet selection described previously, but lowering the \pt threshold to
15 GeV, we consider the closest jet to each lepton within an R $<$ 0.5 of the lepton. The variables include the ratio of the jet to the ratio
of the lepton, the jet \pt, the jet CSVv2 value, the number of charged tracks inside the jet, and pt of the lepton relative to the pt of the jet
($p_{T}^{rel}$), which is defined in equation~\ref{eqn:ptrel} below. 

\begin{equation}
\label{eqn:ptrel}
 p_{T}^{rel} = \frac{(\vec{p}(jet)-\vec{p}(l))\cdot\vec{p}(l)}{||\vec{p}(jet)-\vec{p}(l)||}
\end{equation}

We apply a modified version of the jet energy corrections described previously to avoid biasing the 
prompt lepton selection by over-correcting the jets near the lepton.
%We therefore only correct the hadronic part of the jet according to $jet = l + (jet-PU-l)\times~JEC - PU$.  

\subsubsection{Lepton MVA}
In addition to the above selection requirements, we use a multivariate discriminator designed to select prompt leptons and reject fakes.
This MVA is called the lepton MVA and is based on a boosted decision tree classifier. We use two versions of this MVA, one specifically for
muons and one for electrons. The lepton MVA is trained on prompt leptons in \tth
MC as signal, and the background sample consists of fake leptons from \ttbar MC sample. The input variables are: 

\begin{itemize}
  \item lepton \pt
  \item lepton $|\eta|$
  \item number of charged tracks
  \item charged component of mini isolation
  \item neutral component of mini isolation ($\rho$ corrected)
  \item jet $p_{T}^{rel}$
  \item min(jet \pt ratio, 1.5)
  \item jet CSVv2
  \item SIP$_{3D}$
  \item log$|d_{xy}|$
  \item log$|d_{z}|$
  \item segment compatibility (muons only)
  \item electron MVA ID (electrons only)
\end{itemize}

%The lepton MVA used here is the same as the one used in the 2015 \tth analysis~\cite{CMS-PAS-HIG-15-008}.
Passing the tight working point of the lepton MVA (MVA $>$ 0.9) is the defining characteristic of the tight leptons used in the signal regions. 

\subsubsection{Lepton Selection}
The lepton selection consists of three increasingly selective classes: loose, fakeable, and tight. The loose is a preselection, while the
fakeable object selection, which is a tighter subset of the loose, is used to define control regions for estimating the background due to fake leptons.
The tight leptons, which are a subset of the fakeable, define the leptons used in the signal regions of this analysis. The details of these selections are
described in tables~\ref{tab:muonIDs}~and~\ref{tab:eleIDs}.

%%\begin{table}[h!]
\begin{table}[htbp]
\centering
\small
\topcaption{
\label{tab:muonIDs}
Requirements on each of the three muon selections. A few extra requirements are applied for fakeable objects that fail the lepton MVA requirement,
to better control the extrapolation in fragmentation and flavor composition and are marked with a $\dagger$.}
\begin{tabular}{c|c|c|c}
\hline
\bf{Cut} & \bf{Loose} & \bf{Fakeable} & \bf{Tight} \\
\hline
$|\eta| < 2.4$ & \checkmark & \checkmark & \checkmark \\
$\pt$ & $>5$ & $>15$ & $>15$\\
$|d_{xy}| < 0.05$ (cm) & \checkmark & \checkmark & \checkmark \\
$|d_z| < 0.1$ (cm) & \checkmark & \checkmark & \checkmark \\
$\text{SIP}_{3D} < 8$ & \checkmark & \checkmark & \checkmark \\
\miniIso $< 0.4$ & \checkmark & \checkmark & \checkmark \\
is Loose Muon & \checkmark & \checkmark & \checkmark \\
\ptRatio & -- & $>0.5\dagger$ / -- &  -- \\
segmentCompatibility & -- & $>0.3\dagger$ / -- &  -- \\
%%jet CSV  & -- & $< 0.8484$ & $ < 0.8484$ \\
jet CSV  & -- & $< 0.3 \dagger$ / $< 0.8484$ & $ < 0.8484$ \\
is Medium Muon & -- & -- & \checkmark \\
tight-charge & -- & -- & \checkmark \\
lepMVA $> 0.90$ & -- & -- & \checkmark \\
\hline
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\small
\topcaption{
\label{tab:eleIDs}
Requirements on each of the three electron selections. In some cases, the cut values change for different $\eta$ ranges.
These ranges are $0 < |\eta| < 0.8$, $0.8 < |\eta| < 1.479$, and $1.479 < |\eta| < 2.5$ and the respective cut values are given in the form
(value$_1$, value$_2$, value$_3$). Cuts marked with $\dagger$ are applied only to objects failing the tight selection.
}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{c|c|c|c}
\hline
\bf{Cut} & \bf{Loose} & \bf{Fakeable} & \bf{Tight} \\
\hline
$|\eta| < 2.5$ & \checkmark & \checkmark & \checkmark \\
$\pt$ & $>7$ & $>15$ & $>15$ \\
$|d_{xy}| < 0.05$ (cm) & \checkmark & \checkmark & \checkmark \\
$|d_z| < 0.1$ (cm) & \checkmark & \checkmark & \checkmark \\
$\text{SIP}_{3D} < 8$ & \checkmark & \checkmark & \checkmark \\
\miniIso $< 0.4$ & \checkmark & \checkmark & \checkmark \\
MVA ID $> (0.0, 0.0, 0.7)$ & \checkmark & \checkmark & \checkmark \\
$\sigma_{i\eta i\eta} <(0.011,0.011,0.030)$ & -- & \checkmark & \checkmark \\ %& for corr. $\pt>30$ & for corr. $\pt>30$ \\
H/E $< (0.10,0.10,0.07)$ & -- & \checkmark & \checkmark \\ %& for corr. $\pt>30$ & for corr. $\pt>30$ \\
$\Delta\eta_{\textrm in} < (0.01, 0.01, 0.008)$ & -- & \checkmark & \checkmark \\ %& for corr. $\pt>30$ & for corr. $\pt>30$ \\
$\Delta\phi_{\textrm in} < (0.04, 0.04, 0.07)$ & -- & \checkmark & \checkmark \\ %& for corr. $\pt>30$ & for corr. $\pt>30$ \\
$-0.05 < 1/E-1/p < (0.010,0.010,0.005)$ & -- & \checkmark & \checkmark \\ %& for corr. $\pt>30$ & for corr. $\pt>30$ \\
\ptRatio & -- & $>0.5\dagger$ / -- & -- \\
jet CSV  & -- & $< 0.3 \dagger$ / $< 0.8484$ & $ < 0.8484$ \\
tight-charge & -- & -- & \checkmark \\
conversion rejection & -- & -- & \checkmark \\
Number of missing hits & $<2$ & $== 0$ & $== 0$ \\
lepMVA $> 0.90$ & -- & -- & \checkmark \\
\hline
\end{tabular}}
\end{table}

%% \subsubsection{Lepton Variable Validation}
%% COMING SOON...

\subsubsection{Lepton Effiency Scale Factors}
The rate at which objects pass the described lepton selections (known as efficiency) differs slightly between data and MC. Because we use MC predictions and compare directly to data,
the lepton efficiency in MC is corrected to match the lepton efficiency in data. Because we utilize two lepton selections (loose, tight) for the MC predictions, we apply scale
factors derived uniquely for each selection. Because the tight selection is a subset of the loose, all MC events with tight leptons have both loose and tight scale factors applied.
This is because the tight lepton efficiency is measured relative to the loose denominator. 

These corrections are applied as a function of \pt and $|\eta|$ for electrons and muons
separately. These scale factors are applied to MC and defined in equation~\ref{eqn:lepEffSf} below:

\begin{equation}
\label{eqn:lepEffSf}
 SF(p_{T},\eta) = \frac{\varepsilon_{data}(p_{T},\eta)}{\varepsilon_{MC}(p_{T},\eta)}
\end{equation}

\noindent where $\varepsilon(p_{T},\eta)$ is the efficiency measured for a single lepton to pass the selection measured in either data or MC. The total scale factor is the
product of each individual lepton scale factor and is applied on an event-basis.

The tight lepton efficiencies were measured in a control region enriched in Drell-Yan (DY)\footnote{DY events are Z/$\gamma*\rightarrow~l^{\pm}l^{\mp}$, where $l=e,\mu$.} events on MC using
the tag-and-probe method\footnote{The tag-and-probe method is a technique used throughout particle physics experiments to measure efficiencies of objects in data in an unbiased way by exploiting known
di-object resonances. The pairs from the resonances are reconstructed requiring one object to pass a ``tight'' selection (tag) and the other to pass a ``loose'' selection (probe). Because
the efficiency must be measured in data (in addition to MC), a trigger is required to collect the data. The trigger fires on the tag leg. The selection of resulting probes are completely unbiased
by any selection present in the trigger. The efficiency is then measured by the ratio of probe objects passing some tight definition in the numerator, to the total probes in the denominator.}. 
Loose leptons form the denominator of the tight efficiency measurements. The measured tight lepton efficiencies are in Figures~\ref{fig:ele_eff} and~\ref{fig:mu_eff}.

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.49\textwidth]{ch4_figs/tnp_eff_eb_2lss_pt.pdf}
   \includegraphics[width=0.49\textwidth]{ch4_figs/tnp_eff_ee_2lss_pt.pdf}
   \caption[Tight electron efficiencies in barrel and endcap]{The efficiency to select tight electrons (from loose) in the barrel (left) and the endcap (right).}
   \label{fig:ele_eff}
   \end{center}
\end{figure}

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.49\textwidth]{ch4_figs/tnp_eff_mb_2lss_pt.pdf}
   \includegraphics[width=0.49\textwidth]{ch4_figs/tnp_eff_me_2lss_pt.pdf}
   \caption[Tight muon efficiencies in barrel and endcap]{The efficiency to select tight muons (from loose) in the barrel (left) and the endcap (right).}
   \label{fig:mu_eff}
   \end{center}
\end{figure}

The data/MC scale factors associated with the efficiency are in Figure~\ref{fig:tightLepEff} below. The loose efficiency scale factors were derived
by the SUSY lepton scale factor working group within CMS.

\begin{figure}[hbtp]
 \begin{center}
   \includegraphics[width=0.95\textwidth]{ch4_figs/tight_mu_lepEff_SF.pdf}
   \includegraphics[width=0.95\textwidth]{ch4_figs/tight_ele_lepEff_SF.pdf}
   \caption[Lepton selection efficiency Data/MC scale factors]{The loose-to-tight efficiency data/MC scale factors in bins of \pt and $|\eta|$ for individual muons (top) and electrons (bottom).}
   \label{fig:tightLepEff}
   \end{center}
\end{figure}


\subsection{Taus}
The final objects considered in this analysis are tau leptons, specifically hadronically decaying taus. Leptonic tau decays produce a charged lepton that is typically identified as prompt,
making their presence straightforward to detect, but impossible to distinguish from a lepton produced directly from a W or Z. To avoid an overlap with the signal regions of the \tth,$H\rightarrow\tau\tau$
analysis~\cite{CMS-PAS-HIG-17-003}, which considers \emph{only} events with hadronic taus, we use the same object definition and veto any event with a selected hadronic tau. Hadronic taus are
reconstructed with the hadron-plus-strips algorithm~\cite{hps} in 1-prong or 3-prong decay modes. Taus must have \pt $>$ 20 GeV, $|\eta|<$2.3, $d_{xy}<$1000 cm, $d_{z}<$0.2 cm, and pass the
``decay mode finding'' MVA. Additonally, taus are required to pass the medium working point of a tau MVA\footnote{The technical name for this working point is``byMediumIsolationMVArun2v1DBdR03oldDMwLT''.}
which was trained on \tth events as signal and \ttbar events as background with an isolation cone of radius 0.3~\cite{CMS-AN-15-310}. 

\section{Object Cleaning}
\label{sec:cleaning}
Objects that pass the above selections but are spatially nearby must be addressed since they can be
the decay products of the same underlying particle, but resolved by the detector as two distinct objects. This results in \emph{double-counting}.
We address this by removing one of the overlapping objects, which is decided according to the object type in a process called object cleaning. By correctly choosing which objects to remove, and which objects to keep,
we can mitigate the effects of double-counting. The first object cleaning is applied to leptons passing the loose selection. Loose electrons are removed\footnote{Because the lepton selection is applied in
order, any objects removed after the loose selection are completely removed from the event and not considered for fakeable and tight.} when they overlap within a cone size of R = 0.05 of a loose muon. 
Jets are removed when they overlap within a cone of radius R = 0.4 with fakeable lepton (muon or electron) or a tau passing the cleaning selection.
